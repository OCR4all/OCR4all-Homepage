(window.webpackJsonp=window.webpackJsonp||[]).push([[65],{461:function(e,n,r){"use strict";r.r(n);var i=r(54),t=Object(i.a)({},(function(){var e=this,n=e.$createElement,r=e._self._c||n;return r("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[r("h3",{attrs:{id:"_4-9training"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_4-9training"}},[e._v("#")]),e._v(" 4.9\tTraining")]),e._v(" "),r("p",[r("strong",[e._v("Input")]),e._v(": Zeilenbilder mit entsprechender Ground Truth sowie optional bereits bestehende OCR-Modelle, die als sog. Pretraining und Datengrundlage des Modelltrainings genutzt werden"),r("br"),e._v(" "),r("strong",[e._v("Output")]),e._v(": ein oder mehrere OCR-Modelle")]),e._v(" "),r("p",[e._v("Generell muss es das Ziel sein, einen insgesamt möglichst fehlerfreien Text zu erhalten!"),r("br"),e._v("\nWarum dann aber die Erstellung werkspezifischer Modelle mittels des Trainings-Moduls statt einfacher, abschließender Textkorrektur?")]),e._v(" "),r("p",[e._v("→ Je besser das Modell, welches zur Texterkennung genutzt wird, desto kürzere fällt die Korrekturzeit aus. Idee und Sinn eines kontinuierlichen Modelltrainings sind es also, mit fortlaufendem Korrekturfortschritt auch immer bessere Modelle zu trainieren und somit den Korrekturaufwand für die danach noch zu korrigierenden Seiten des Werkes auf ein Minimum zu reduzieren.")]),e._v(" "),r("ul",[r("li",[e._v("Innerhalb des Trainingstools können auf Grundlage aller zu einem Werk vorliegenden Zeilen Ground Truth werkspezifische Modelle bzw. -ensembles trainiert werden. Dazu werden in den allgemeinen Einstellungen folgende Werte eingetragen:\n"),r("ul",[r("li",[e._v("„"),r("strong",[e._v("The number of folds")]),e._v(" (= the number of models) "),r("strong",[e._v("to train")]),e._v("“: "),r("strong",[e._v("5")]),e._v(" → Es wird im Folgenden ein Modellensemble, bestehend aus fünf Einzelmodellen, trainiert.")]),e._v(" "),r("li",[e._v("„"),r("strong",[e._v("Only train a single fold")]),e._v(" (= a single model)“: "),r("em",[e._v("Nichts eintragen!")]),e._v(" → Es werden alle fünf Einzelmodelle statt nur eines einzelnen trainiert.")]),e._v(" "),r("li",[e._v("„"),r("strong",[e._v("Number of models to train in parallel")]),e._v("“: "),r("strong",[e._v("-1")]),e._v(" → Alle Modelle des Ensembles werden gleichzeitig trainiert.")]),e._v(" "),r("li",[e._v("„"),r("strong",[e._v("Keep all characters loaded from the last model")]),e._v("”: Auswählen, wenn alle in den „Pretraining”-Modellen enthaltenen Zeichen im zu trainierenden Modell beibehalten werden sollen, also zu dessen Whitelist hinzugefügt werden.")]),e._v(" "),r("li",[e._v("„"),r("strong",[e._v("Whitelist characters to keep in the model")]),e._v("”: Liste von Zeichen, die beim Training und im daraus entstehenden Modell berücksichtigt werden. Alle Zeichen außerhalb dieser „Whitelist“ werden nicht berücksichtigt.")]),e._v(" "),r("li",[e._v("„Pretraining“:"),r("br"),e._v("\n„"),r("strong",[e._v("Train each model based on different existing models")]),e._v("“ (Im Folgenden öffnen sich fünf Dropdown-Listen; in jede wird eines der gemischten Modelle des Modellensembles eingetragen, das wie empfohlen zur ersten Erkennung von Text im vorliegenden Werk genutzt wurde; egal bei welcher Trainingsiteration der Nutzer steht: Auch wenn bspw. bereits das dritte werkspezifische Modell trainiert wird – es werden trotzdem immer die fünf zu Beginn verwendeten grundlegenden gemischten Modelle eingetragen)"),r("br"),e._v(" "),r("strong",[e._v("ODER")]),r("br"),e._v("\n„"),r("strong",[e._v("Train all models based on one existing model")]),e._v("“ (Wurde die erste Texterkennung auf Grundlage eines einzelnen gemischten Modells durchgeführt, so wird nur ein Modell eingetragen; jedoch gilt auch hier, dass bei jeder Iteration eben dieses gemischte Modell erneut angegeben werden muss).")]),e._v(" "),r("li",[e._v("„"),r("strong",[e._v("Data augmentation")]),e._v("“: "),r("em",[e._v("Nichts eintragen.")]),e._v(" → Aber: beschreibt die Anzahl der Datenerweiterungen pro Zeile. Es kann hier ein Wert, bspw. 5, angegeben werden, um damit die Menge des Trainingsmaterials zu erhöhen, auf der trainiert wird. Dies kann zur Erstellung besserer Modelle führen, benötigt aber deutlich mehr Trainingszeit.")]),e._v(" "),r("li",[e._v("„"),r("strong",[e._v("Skip retraining on real data only")]),e._v("“: "),r("em",[e._v("Nicht auswählen!")])])])]),e._v(" "),r("li",[e._v("Die erweiterten Einstellungen bleiben unverändert.")])]),e._v(" "),r("p",[r("img",{attrs:{src:"/docs/.vuepress/public/images/Abb39.png",alt:"Abb39.png"}})]),e._v(" "),r("p",[e._v("Abb. 39: Einstellungen für das Training von werkspezifischen Modellensembles.")]),e._v(" "),r("ul",[r("li",[e._v("Mittels „EXECUTE“ wird das Training gestartet. Im Folgenden kann das Training der Konsole nachvollzogen werden. Je nach Gesamtmenge der vorhandenen Zeilen Ground Truth variieren die Trainingszeiten.")]),e._v(" "),r("li",[e._v("Entsprechend obiger Einstellungen wird durch das Training ein werkspezifisches Modellensemble, bestehend aus fünf Einzelmodellen, erstellt, welches in ocr4all/models/Werktitel/0 gespeichert wird. Das Modellensemble trägt folglich den Namen „0“. Es kann nun, zur weiteren Arbeit am vorliegenden Werk und Verbesserung der Erkennung innerhalb des Menüpunkts „Recognition“ und der Spalte der auswählbaren Modelle, zur Erkennung neuer Textseiten verwendet werden. Soll ein zweites werkspezifisches Modellensemble erstellt werden, mit Hilfe dessen bspw. mögliche Schwächen des ersten behoben werden können, wird erneut vorgegangen wie hier beschrieben. Dem neuen werkspezifischen Modell wird dann automatisch die Bezeichnung „1“ zugewiesen. Die Bezeichnungen weiterer Modellensembles setzt sich nach diesem Schema fort.")])])])}),[],!1,null,null,null);n.default=t.exports}}]);