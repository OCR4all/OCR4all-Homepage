import{_ as e,e as n}from"./app.b397120f.js";import{_ as r}from"./Abb39.fcf1d8c5.js";const i={},t=n('<h3 id="_4-9training" tabindex="-1"><a class="header-anchor" href="#_4-9training" aria-hidden="true">#</a> 4.9 Training</h3><p><strong>Input</strong>: Zeilenbilder mit entsprechender Ground Truth sowie optional bereits bestehende OCR-Modelle, die als sog. Pretraining und Datengrundlage des Modelltrainings genutzt werden<br><strong>Output</strong>: ein oder mehrere OCR-Modelle</p><p>Generell muss es das Ziel sein, einen insgesamt m\xF6glichst fehlerfreien Text zu erhalten!<br> Warum dann aber die Erstellung werkspezifischer Modelle mittels des Trainings-Moduls statt einfacher, abschlie\xDFender Textkorrektur?</p><p>\u2192 Je besser das Modell, welches zur Texterkennung genutzt wird, desto k\xFCrzere f\xE4llt die Korrekturzeit aus. Idee und Sinn eines kontinuierlichen Modelltrainings sind es also, mit fortlaufendem Korrekturfortschritt auch immer bessere Modelle zu trainieren und somit den Korrekturaufwand f\xFCr die danach noch zu korrigierenden Seiten des Werkes auf ein Minimum zu reduzieren.</p><ul><li>Innerhalb des Trainingstools k\xF6nnen auf Grundlage aller zu einem Werk vorliegenden Zeilen Ground Truth werkspezifische Modelle bzw. -ensembles trainiert werden. Dazu werden in den allgemeinen Einstellungen folgende Werte eingetragen: <ul><li>\u201E<strong>The number of folds</strong> (= the number of models) <strong>to train</strong>\u201C: <strong>5</strong> \u2192 Es wird im Folgenden ein Modellensemble, bestehend aus f\xFCnf Einzelmodellen, trainiert.</li><li>\u201E<strong>Only train a single fold</strong> (= a single model)\u201C: <em>Nichts eintragen!</em> \u2192 Es werden alle f\xFCnf Einzelmodelle statt nur eines einzelnen trainiert.</li><li>\u201E<strong>Number of models to train in parallel</strong>\u201C: <strong>-1</strong> \u2192 Alle Modelle des Ensembles werden gleichzeitig trainiert.</li><li>\u201E<strong>Keep all characters loaded from the last model</strong>\u201D: Ausw\xE4hlen, wenn alle in den \u201EPretraining\u201D-Modellen enthaltenen Zeichen im zu trainierenden Modell beibehalten werden sollen, also zu dessen Whitelist hinzugef\xFCgt werden.</li><li>\u201E<strong>Whitelist characters to keep in the model</strong>\u201D: Liste von Zeichen, die beim Training und im daraus entstehenden Modell ber\xFCcksichtigt werden. Alle Zeichen au\xDFerhalb dieser \u201EWhitelist\u201C werden nicht ber\xFCcksichtigt.</li><li>\u201EPretraining\u201C:<br> \u201E<strong>Train each model based on different existing models</strong>\u201C (Im Folgenden \xF6ffnen sich f\xFCnf Dropdown-Listen; in jede wird eines der gemischten Modelle des Modellensembles eingetragen, das wie empfohlen zur ersten Erkennung von Text im vorliegenden Werk genutzt wurde; egal bei welcher Trainingsiteration der Nutzer steht: Auch wenn bspw. bereits das dritte werkspezifische Modell trainiert wird \u2013 es werden trotzdem immer die f\xFCnf zu Beginn verwendeten grundlegenden gemischten Modelle eingetragen)<br><strong>ODER</strong><br> \u201E<strong>Train all models based on one existing model</strong>\u201C (Wurde die erste Texterkennung auf Grundlage eines einzelnen gemischten Modells durchgef\xFChrt, so wird nur ein Modell eingetragen; jedoch gilt auch hier, dass bei jeder Iteration eben dieses gemischte Modell erneut angegeben werden muss).</li><li>\u201E<strong>Data augmentation</strong>\u201C: <em>Nichts eintragen.</em> \u2192 Aber: beschreibt die Anzahl der Datenerweiterungen pro Zeile. Es kann hier ein Wert, bspw. 5, angegeben werden, um damit die Menge des Trainingsmaterials zu erh\xF6hen, auf der trainiert wird. Dies kann zur Erstellung besserer Modelle f\xFChren, ben\xF6tigt aber deutlich mehr Trainingszeit.</li><li>\u201E<strong>Skip retraining on real data only</strong>\u201C: <em>Nicht ausw\xE4hlen!</em></li></ul></li><li>Die erweiterten Einstellungen bleiben unver\xE4ndert.</li></ul><p><img src="'+r+'" alt="Abb39.png"></p><p>Abb. 39: Einstellungen f\xFCr das Training von werkspezifischen Modellensembles.</p><ul><li>Mittels \u201EEXECUTE\u201C wird das Training gestartet. Im Folgenden kann das Training der Konsole nachvollzogen werden. Je nach Gesamtmenge der vorhandenen Zeilen Ground Truth variieren die Trainingszeiten.</li><li>Entsprechend obiger Einstellungen wird durch das Training ein werkspezifisches Modellensemble, bestehend aus f\xFCnf Einzelmodellen, erstellt, welches in ocr4all/models/Werktitel/0 gespeichert wird. Das Modellensemble tr\xE4gt folglich den Namen \u201E0\u201C. Es kann nun, zur weiteren Arbeit am vorliegenden Werk und Verbesserung der Erkennung innerhalb des Men\xFCpunkts \u201ERecognition\u201C und der Spalte der ausw\xE4hlbaren Modelle, zur Erkennung neuer Textseiten verwendet werden. Soll ein zweites werkspezifisches Modellensemble erstellt werden, mit Hilfe dessen bspw. m\xF6gliche Schw\xE4chen des ersten behoben werden k\xF6nnen, wird erneut vorgegangen wie hier beschrieben. Dem neuen werkspezifischen Modell wird dann automatisch die Bezeichnung \u201E1\u201C zugewiesen. Die Bezeichnungen weiterer Modellensembles setzt sich nach diesem Schema fort.</li></ul>',8);function l(s,d){return t}var g=e(i,[["render",l],["__file","4.009-Training.html.vue"]]);export{g as default};
