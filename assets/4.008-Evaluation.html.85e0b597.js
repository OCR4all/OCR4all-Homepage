import{_ as e,e as n}from"./app.b397120f.js";import{_ as r}from"./Abb38.47dfc288.js";const t={},i=n('<h3 id="_4-8evaluation" tabindex="-1"><a class="header-anchor" href="#_4-8evaluation" aria-hidden="true">#</a> 4.8 Evaluation</h3><p><strong>Input</strong>: zeilenbasierte OCR-Texte und entsprechende Ground Truth<br><strong>Output</strong>: Fehlerstatistiken</p><ul><li>Der Men\xFCpunkt Evaluation dient der Ermittlung der Erkennungsrate eines aktuell verwendeten Modells (gemischt oder werkspezifisch).</li><li>Um diese zu generieren, werden all jene Scans in der rechten Seitenleiste ausgew\xE4hlt, die mittels dieses aktuellen Modells erkannt und danach in der \u201EGround Truth Production\u201D korrigiert wurden. Klickt der Nutzer auf \u201EEXECUTE\u201D und l\xE4sst s\xE4mtliche Einstellungen unver\xE4ndert, so wird ihm in der Konsole eine Tabelle ausgegeben: Ganz oben in der Ausgabe finden sich als Prozentsatz die Fehlerrate sowie die Gesamtanzahl der Fehler (\u201Eerrs\u201D). Darunter werden \u2013 tabellarisch gelistet durch den Vergleich von Ausgabetext der Recognition und w\xE4hrend der Korrektur erstellter Ground Truth \u2013 die gefundenen Fehler angezeigt. In der ersten Spalte ist dabei der korrigierte Text zu erkennen (\u201EGT\u201D), in der zweiten Spalte der urspr\xFCnglich durch das Modell erkannte (\u201EPRED\u201D), dahinter die H\xE4ufigkeit des Auftretens genau jenes Fehlers sowie der Prozentsatz eben dieses Fehlers an der Gesamtfehlermenge.</li></ul><p><img src="'+r+'" alt="Abb38.png"></p><p>Abb. 38: Evaluationsergebnis mit Gesamtfehlerrate und den zehn h\xE4ufigsten Fehler sowie deren Prozentsatz an der Gesamtfehlermenge.</p><ul><li>Mittels dieser tabellarischen Listung sowie der Erkennungsrate (100% - Fehlerrate) kann nun durch den Nutzer die Absch\xE4tzung \xFCber die Sinnhaftigkeit eines (neuerlichen) Trainings von werkspezifischen Modellen erfolgen.</li></ul>',6);function s(a,l){return i}var o=e(t,[["render",s],["__file","4.008-Evaluation.html.vue"]]);export{o as default};
